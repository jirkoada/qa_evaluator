{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5160c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11b717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cli_evaluate import *\n",
    "from cli_create_dataset import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f3095",
   "metadata": {},
   "source": [
    "Test parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "301bbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = \"example_vs\"\n",
    "# Path or None\n",
    "# If a path is provided, params file_paths, chunk_chars, overlap, and split_method will be overridden by the vector \n",
    "# store. Param embeddings needs to be manually set to correspond with the vector store embeddings.\n",
    "\n",
    "file_paths = [\"docs/fee_brochure.pdf\", \"docs/fee_rules\"]\n",
    "\n",
    "dataset_file = \"datasets/example.csv\"\n",
    "# Path or None\n",
    "\n",
    "num_eval_questions = 10\n",
    "\n",
    "chunk_chars = 1500\n",
    "\n",
    "overlap = 0\n",
    "\n",
    "split_method = \"RecursiveTextSplitter\"\n",
    "# Available options:\n",
    "    # \"RecursiveTextSplitter\"\n",
    "    # \"CharacterTextSplitter\"\n",
    "    \n",
    "embeddings = \"OpenAI\"\n",
    "# Available options:\n",
    "    # \"OpenAI\"\n",
    "    # \"gte-large\"\n",
    "    # \"FastText\"\n",
    "    # Hugging Face path, e.g. \"thenlper/gte-large\"\n",
    "    \n",
    "model_version = \"gpt-3.5-turbo\"\n",
    "# Available options:\n",
    "    # \"gpt-3.5-turbo\"\n",
    "    # \"gpt-4\"\n",
    "    # \"llama2\"\n",
    "    \n",
    "grade_prompt = \"OpenAI grading prompt\"\n",
    "# Available options:\n",
    "    # \"Fast\" - only Correct/Incorrect\n",
    "    # \"Descriptive w/ bias check\"\n",
    "    # \"OpenAI grading prompt\" - descriptive w/o bias check\n",
    "    # \"Basic\" - Correctness + basic justification\n",
    "        \n",
    "num_neighbors = 3\n",
    "# Number of retrieved source documents\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"file_paths\": file_paths,\n",
    "    \"dataset_file\": dataset_file,\n",
    "    \"num_eval_questions\": num_eval_questions,\n",
    "    \"chunk_chars\": chunk_chars,\n",
    "    \"overlap\": overlap,\n",
    "    \"split_method\": split_method,\n",
    "    \"embeddings\": embeddings,\n",
    "    \"model_version\": model_version,\n",
    "    \"grade_prompt\": grade_prompt,\n",
    "    \"num_neighbors\": num_neighbors,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cb3b3",
   "metadata": {},
   "source": [
    "Test logic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a93a0c",
   "metadata": {},
   "source": [
    "Load or generate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45114c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 questions\n"
     ]
    }
   ],
   "source": [
    "if dataset_file is None:\n",
    "    test_dataset = create_dataset(file_paths, num_eval_questions, 3000)\n",
    "else:        \n",
    "    test_dataset = pd.read_csv(dataset_file)[[\"question\", \"answer\"]].to_dict('records')\n",
    "    print(f\"Loaded {len(test_dataset)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c1f59",
   "metadata": {},
   "source": [
    "Save dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aaac133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(test_dataset)\n",
    "dataset_df.to_csv(\"datasets/name.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71677167",
   "metadata": {},
   "source": [
    "Load or create vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b02a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded example_vs\n"
     ]
    }
   ],
   "source": [
    "embd = make_embeddings(embeddings)\n",
    "if vector_store is not None:\n",
    "    vs = FAISS.load_local(vector_store, embd)\n",
    "    print(f\"Loaded {vector_store}\")\n",
    "else:\n",
    "    splits = []\n",
    "    for path in file_paths:\n",
    "        loader = PyMuPDFLoader(path) # Fast, Good for metadata\n",
    "        if split_method == \"RecursiveTextSplitter\":\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_chars,\n",
    "                                                           chunk_overlap=overlap)\n",
    "        elif split_method == \"CharacterTextSplitter\":\n",
    "            text_splitter = CharacterTextSplitter(separator=\" \",\n",
    "                                                  chunk_size=chunk_chars,\n",
    "                                                  chunk_overlap=overlap)\n",
    "        else:\n",
    "            raise Exception(\"Invalid text splitter\")\n",
    "    \n",
    "        local_pages = loader.load_and_split(text_splitter)\n",
    "        splits.extend(local_pages)\n",
    "    vs = FAISS.from_documents(splits, embd)\n",
    "    print(\"New vector store created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2696ab7",
   "metadata": {},
   "source": [
    "Save vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4598264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.save_local(\"vs_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb535de9",
   "metadata": {},
   "source": [
    "Run test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a939fb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Question 10 / 10, Correct: 6 / 9, ACC: 0.6666666666666666\n",
      "\n",
      "Total score: 0.6 (6 / 10)\n",
      "Avg. latency: 1.6852789454989963\n"
     ]
    }
   ],
   "source": [
    "retriever = vs.as_retriever(k=num_neighbors)\n",
    "results = run_evaluator(retriever, test_dataset, num_eval_questions, model_version, grade_prompt)\n",
    "score = results[\"answerScore\"].mean()\n",
    "score_str = f\"Total score: {score} ({len(results['answerScore'][results['answerScore'] == 1])} / {len(results['answerScore'])})\"\n",
    "print(\"\\n\")\n",
    "print(score_str)\n",
    "lat = results[results['latency'] < results['latency'].quantile(0.99)]['latency'].mean()\n",
    "print(f\"Avg. latency: {lat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a0543",
   "metadata": {},
   "source": [
    "Show incorrect examples first (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4cf43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values(\"answerScore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6924eb6",
   "metadata": {},
   "source": [
    "Save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae2bebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[score_str] = \"\"\n",
    "results[f\"Parameters: {str(params)}\"] = \"\"\n",
    "results.loc[results.index[0], score_str] = f\"Typical latency: {lat}\"\n",
    "results.to_csv(\"results/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd5d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
