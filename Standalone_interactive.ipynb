{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5160c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11b717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import itertools\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from json import JSONDecodeError\n",
    "from langchain.llms import MosaicML\n",
    "from langchain.llms import Anthropic\n",
    "from langchain.llms import Replicate\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import QAGenerationChain\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, PyMuPDFLoader\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.embeddings import MosaicMLInstructorEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from text_utils import GRADE_DOCS_PROMPT, GRADE_ANSWER_PROMPT, GRADE_DOCS_PROMPT_FAST, GRADE_ANSWER_PROMPT_FAST, GRADE_ANSWER_PROMPT_BIAS_CHECK, GRADE_ANSWER_PROMPT_OPENAI, QA_CHAIN_PROMPT, QA_CHAIN_PROMPT_LLAMA\n",
    "from text_utils import GENERATION_PROMPT_SELECTOR\n",
    "from integrations import FTembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1073c457-dcf9-4c92-84a4-e9a0c4a898f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "print(cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f3095",
   "metadata": {},
   "source": [
    "Test parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301bbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = \"example_vs\"\n",
    "# Path or None\n",
    "# If a path is provided, params file_paths, chunk_chars, overlap, and split_method will be overridden by the vector \n",
    "# store. Param embeddings needs to be manually set to correspond with the vector store embeddings.\n",
    "\n",
    "file_paths = [\"docs/fee_brochure.pdf\", \"docs/fee_rules\"]\n",
    "\n",
    "dataset_file = \"datasets/example.csv\"\n",
    "# Path or None\n",
    "\n",
    "num_eval_questions = 10\n",
    "\n",
    "chunk_chars = 1500\n",
    "\n",
    "overlap = 0\n",
    "\n",
    "split_method = \"RecursiveTextSplitter\"\n",
    "# Available options:\n",
    "    # \"RecursiveTextSplitter\"\n",
    "    # \"CharacterTextSplitter\"\n",
    "    \n",
    "embeddings = \"OpenAI\"\n",
    "# Available options:\n",
    "    # \"OpenAI\"\n",
    "    # \"gte-large\"\n",
    "    # \"FastText\"\n",
    "    # Hugging Face path, e.g. \"thenlper/gte-large\"\n",
    "    \n",
    "model_version = \"gpt-3.5-turbo\"\n",
    "# Available options:\n",
    "    # \"gpt-3.5-turbo\"\n",
    "    # \"gpt-4\"\n",
    "    # \"llama2\"\n",
    "    \n",
    "grade_prompt = \"Basic\"\n",
    "# Available options:\n",
    "    # \"Fast\" - only Correct/Incorrect\n",
    "    # \"Descriptive w/ bias check\"\n",
    "    # \"OpenAI grading prompt\" - descriptive w/o bias check\n",
    "    # \"Basic\" - Correctness + basic justification\n",
    "        \n",
    "num_neighbors = 3\n",
    "# Number of retrieved source documents\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"file_paths\": file_paths,\n",
    "    \"dataset_file\": dataset_file,\n",
    "    \"num_eval_questions\": num_eval_questions,\n",
    "    \"chunk_chars\": chunk_chars,\n",
    "    \"overlap\": overlap,\n",
    "    \"split_method\": split_method,\n",
    "    \"embeddings\": embeddings,\n",
    "    \"model_version\": model_version,\n",
    "    \"grade_prompt\": grade_prompt,\n",
    "    \"num_neighbors\": num_neighbors,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cb3b3",
   "metadata": {},
   "source": [
    "Test logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8719403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama_pipeline(model_id='meta-llama/Llama-2-7b-chat-hf', temperature=0.1):\n",
    "    from torch import cuda, bfloat16\n",
    "    from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "    import torch\n",
    "    import transformers\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "    \n",
    "    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "    # set quantization configuration to load large model with less GPU memory\n",
    "    # this requires the `bitsandbytes` library\n",
    "    bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=bfloat16\n",
    "    )\n",
    "\n",
    "    # begin initializing HF items, you need an access token\n",
    "    hf_auth = os.getenv('HF_TOKEN')\n",
    "    model_config = transformers.AutoConfig.from_pretrained(\n",
    "        model_id,\n",
    "        use_auth_token=hf_auth\n",
    "    )\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        use_auth_token=hf_auth\n",
    "    )\n",
    "\n",
    "    # enable evaluation mode to allow model inference\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded on {device}\")\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_auth_token=hf_auth\n",
    "    )\n",
    "    \n",
    "    stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "    stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "    stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "\n",
    "    # define custom stopping criteria object\n",
    "    class StopOnTokens(StoppingCriteria):\n",
    "        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "            for stop_ids in stop_token_ids:\n",
    "                if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "    \n",
    "    generate_text = transformers.pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=True,  # langchain expects the full text\n",
    "        task='text-generation',\n",
    "        # we pass model parameters here too\n",
    "        stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "        temperature=temperature,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        max_new_tokens=1024,  # max number of tokens to generate in the output\n",
    "        repetition_penalty=1.1  # without this output begins repeating\n",
    "    )\n",
    "    \n",
    "    return HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "\n",
    "def make_llm(model):\n",
    "    \"\"\"\n",
    "    Make LLM\n",
    "    @param model: LLM to use\n",
    "    @return: LLM\n",
    "    \"\"\"\n",
    "\n",
    "    if model in (\"gpt-3.5-turbo\", \"gpt-4\"):\n",
    "        # TODO: Try langchain.llms.OpenAI instead\n",
    "        llm = ChatOpenAI(model_name=model, temperature=0)\n",
    "    elif model == \"anthropic\":\n",
    "        llm = Anthropic(temperature=0)\n",
    "    elif model == \"Anthropic-100k\":\n",
    "        llm = Anthropic(model=\"claude-v1-100k\",temperature=0)\n",
    "    elif model == \"vicuna-13b\":\n",
    "        llm = Replicate(model=\"replicate/vicuna-13b:e6d469c2b11008bb0e446c3e9629232f9674581224536851272c54871f84076e\",\n",
    "                input={\"temperature\": 0.75, \"max_length\": 3000, \"top_p\":0.25})\n",
    "    elif model == \"mosaic\":\n",
    "        llm = MosaicML(inject_instruction_format=True,model_kwargs={'do_sample': False, 'max_length': 3000})\n",
    "    elif model == \"llama2\":\n",
    "        #llm = LlamaCpp(model_path=\"/lscratch/poludmik/llama2/from_hf_2-7b/ggml-model-q4_0.bin\", n_ctx=4096)\n",
    "        llm = create_llama_pipeline()\n",
    "    else:\n",
    "        raise Exception(\"Invalid model choice\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "def make_embeddings(embeddings):\n",
    "    if embeddings == \"OpenAI\":\n",
    "        embd = OpenAIEmbeddings()\n",
    "    # Note: Still WIP (can't be selected by user yet)\n",
    "    elif embeddings == \"LlamaCppEmbeddings\":\n",
    "        embd = LlamaCppEmbeddings(model_path=\"/lscratch/poludmik/llama2/from_hf_2-7b/ggml-model-q4_0.bin\", n_ctx=2048)\n",
    "    elif embeddings == \"FastText\":\n",
    "        embd = FTembeddings(\"models/cc.en.300.bin\")\n",
    "    elif embeddings == \"gte-large\":\n",
    "        model_name = \"thenlper/gte-large\"\n",
    "        embd = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    else:\n",
    "        model_name = embeddings\n",
    "        embd = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embd\n",
    "\n",
    "\n",
    "def make_chain(llm, retriever, model):\n",
    "\n",
    "    \"\"\"\n",
    "    Make retrieval chain\n",
    "    @param llm: model\n",
    "    @param retriever: retriever\n",
    "    @return: QA chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Select prompt \n",
    "    if model == \"vicuna-13b\":\n",
    "        # Note: Better answer quality using default prompt \n",
    "        # chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT_LLAMA}\n",
    "        chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT}\n",
    "    else: \n",
    "        chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT}\n",
    "\n",
    "    # Select model \n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                           chain_type=\"stuff\",\n",
    "                                           retriever=retriever,\n",
    "                                           chain_type_kwargs=chain_type_kwargs,\n",
    "                                           input_key=\"question\")\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def grade_model_answer(predicted_dataset, predictions, grade_answer_prompt):\n",
    "    \"\"\"\n",
    "    Grades the answer based on ground truth and model predictions.\n",
    "    @param predicted_dataset: A list of dictionaries containing ground truth questions and answers.\n",
    "    @param predictions: A list of dictionaries containing model predictions for the questions.\n",
    "    @param grade_answer_prompt: The prompt level for the grading. Either \"Fast\" or \"Full\".\n",
    "    @return: A list of scores for the distilled answers.\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"`Grading model answer ...`\")\n",
    "    if grade_answer_prompt == \"Fast\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_FAST\n",
    "    elif grade_answer_prompt == \"Descriptive w/ bias check\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_BIAS_CHECK\n",
    "    elif grade_answer_prompt == \"OpenAI grading prompt\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_OPENAI\n",
    "    else:\n",
    "        prompt = GRADE_ANSWER_PROMPT\n",
    "\n",
    "    # Note: GPT-4 grader is advised by OAI \n",
    "    eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
    "                                      prompt=prompt)\n",
    "    graded_outputs = eval_chain.evaluate(predicted_dataset,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return graded_outputs\n",
    "\n",
    "\n",
    "def grade_model_retrieval(gt_dataset, predictions, grade_docs_prompt):\n",
    "    \"\"\"\n",
    "    Grades the relevance of retrieved documents based on ground truth and model predictions.\n",
    "    @param gt_dataset: list of dictionaries containing ground truth questions and answers.\n",
    "    @param predictions: list of dictionaries containing model predictions for the questions\n",
    "    @param grade_docs_prompt: prompt level for the grading.\n",
    "    @return: list of scores for the retrieved documents.\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"`Grading relevance of retrieved docs ...`\")\n",
    "    if grade_docs_prompt == \"Fast\":\n",
    "        prompt = GRADE_DOCS_PROMPT_FAST\n",
    "    else:\n",
    "        prompt = GRADE_DOCS_PROMPT\n",
    "\n",
    "    # Note: GPT-4 grader is advised by OAI\n",
    "    eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
    "                                      prompt=prompt)\n",
    "    graded_outputs = eval_chain.evaluate(gt_dataset,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return graded_outputs\n",
    "\n",
    "\n",
    "def run_eval(chain, retriever, eval_qa_pair, grade_prompt, num_neighbors, text):\n",
    "    \"\"\"\n",
    "    Runs evaluation on a model's performance on a given evaluation dataset.\n",
    "    @param chain: Model chain used for answering questions\n",
    "    @param retriever:  Document retriever used for retrieving relevant documents\n",
    "    @param eval_set: List of dictionaries containing questions and corresponding ground truth answers\n",
    "    @param grade_prompt: String prompt used for grading model's performance\n",
    "    @param num_neighbors: Number of neighbors to retrieve using the retriever\n",
    "    @param text: full document text\n",
    "    @return: A tuple of four items:\n",
    "    - answers_grade: A dictionary containing scores for the model's answers.\n",
    "    - retrieval_grade: A dictionary containing scores for the model's document retrieval.\n",
    "    - latencies_list: A list of latencies in seconds for each question answered.\n",
    "    - predictions_list: A list of dictionaries containing the model's predicted answers and relevant documents for each question.\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"`Running eval ...`\")\n",
    "    predictions = []\n",
    "    retrieved_docs = []\n",
    "    gt_dataset = []\n",
    "    latency = []\n",
    "\n",
    "    # Get answer and log latency\n",
    "    start_time = time.time()\n",
    "    predictions.append(chain(eval_qa_pair))\n",
    "    gt_dataset.append(eval_qa_pair)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    latency.append(elapsed_time)\n",
    "\n",
    "    # Extract text from retrieved docs\n",
    "    retrieved_doc_text = \"\"\n",
    "    \n",
    "    docs = retriever.get_relevant_documents(eval_qa_pair[\"question\"])\n",
    "    for i, doc in enumerate(docs):\n",
    "        retrieved_doc_text += \"Doc %s: \" % str(i+1) + \\\n",
    "            doc.page_content + \" \"\n",
    "\n",
    "    # Log\n",
    "    retrieved = {\"question\": eval_qa_pair[\"question\"],\n",
    "                 \"answer\": eval_qa_pair[\"answer\"], \"result\": retrieved_doc_text}\n",
    "    retrieved_docs.append(retrieved)\n",
    "\n",
    "    # Grade\n",
    "    graded_answers = grade_model_answer(\n",
    "        gt_dataset, predictions, grade_prompt)\n",
    "    graded_retrieval = grade_model_retrieval(\n",
    "        gt_dataset, retrieved_docs, grade_prompt)\n",
    "    return graded_answers, graded_retrieval, latency, predictions\n",
    "\n",
    "\n",
    "def run_evaluator(retriever, test_dataset, num_eval_questions, model_version, grade_prompt):\n",
    "    print(\"Making LLM\")\n",
    "    llm = make_llm(model_version)\n",
    "\n",
    "    print(\"Making chain\")\n",
    "    qa_chain = make_chain(llm, retriever, model_version)\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    #print()\n",
    "    num_q = min(num_eval_questions, len(test_dataset))\n",
    "    correct = 0\n",
    "    acc = float(\"nan\")\n",
    "    for i in range(num_q):\n",
    "        print(f\"\\rQuestion {i+1} / {num_q}\", end=\"\")\n",
    "        print(f\", Correct: {correct} / {i}, ACC: {acc}\", end=\"\")\n",
    "        eval_pair = test_dataset[i]\n",
    "        \n",
    "        # Run eval\n",
    "        graded_answers, graded_retrieval, latency, predictions = run_eval(\n",
    "            qa_chain, retriever, eval_pair, grade_prompt, num_neighbors, \"\")\n",
    "\n",
    "        # Assemble output\n",
    "        d = pd.DataFrame(predictions)\n",
    "        #d['answerScore'] = [g['text'] for g in graded_answers]\n",
    "        #d['retrievalScore'] = [g['text'] for g in graded_retrieval]\n",
    "        d['answerScore'] = 1 if \"Incorrect\" not in graded_answers[0][\"text\"] else 0\n",
    "        d['answerComment'] = graded_answers[0][\"text\"]\n",
    "        d['retrievalScore'] = 1 if \"Incorrect\" not in graded_retrieval[0][\"text\"] else 0\n",
    "        d['retrievalComment'] = graded_retrieval[0][\"text\"]\n",
    "        d['latency'] = latency\n",
    "        \n",
    "        correct += d['answerScore'][0]\n",
    "        acc = correct / (i+1)\n",
    "\n",
    "        # Convert dataframe to dict\n",
    "        d_dict = d.to_dict('records')\n",
    "        results = pd.concat([results, d], ignore_index=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_dataset(paths, num_questions, context_length):\n",
    "    text = \"\"\n",
    "    for path in paths:\n",
    "        loader = PyMuPDFLoader(path)\n",
    "        local_pages = loader.load_and_split()\n",
    "        for page in local_pages:\n",
    "            text += page.page_content\n",
    "    \n",
    "    pairs = []\n",
    "    section_length = len(text) // num_questions\n",
    "    print(f\"Chars per section: {section_length}\")\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "    prompt = GENERATION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "    chain = QAGenerationChain.from_llm(llm, prompt)\n",
    "    \n",
    "    for i in range(num_questions):\n",
    "        print(f\"\\rGenerating question {i+1} out of {num_questions}\", end=\"\")\n",
    "        start_index = random.randint(0, section_length//2) + i * section_length\n",
    "        sub_sequence = text[start_index : start_index + context_length]\n",
    "        eval_set = []\n",
    "        # Catch any QA generation errors and re-try until QA pair is generated\n",
    "        awaiting_answer = True\n",
    "        while awaiting_answer:\n",
    "            try:\n",
    "                qa_pair = chain.run(sub_sequence)\n",
    "                eval_set.append(qa_pair)\n",
    "                awaiting_answer = False\n",
    "            except Exception as e:\n",
    "                print(\"Exception: \", e)\n",
    "                start_index = random.randint(0, section_length//1.4) + i * section_length\n",
    "                sub_sequence = text[start_index : start_index + context_length]\n",
    "        eval_pair = list(itertools.chain.from_iterable(eval_set))\n",
    "        pairs.append(eval_pair[0])\n",
    "    print()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a93a0c",
   "metadata": {},
   "source": [
    "Load or generate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45114c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 questions\n"
     ]
    }
   ],
   "source": [
    "if dataset_file is None:\n",
    "    test_dataset = create_dataset(file_paths, num_eval_questions, 3000)\n",
    "else:        \n",
    "    test_dataset = pd.read_csv(dataset_file)[[\"question\", \"answer\"]].to_dict('records')\n",
    "    print(f\"Loaded {len(test_dataset)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c1f59",
   "metadata": {},
   "source": [
    "Save dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aaac133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(test_dataset)\n",
    "dataset_df.to_csv(\"datasets/name.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71677167",
   "metadata": {},
   "source": [
    "Load or create vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b02a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded example_vs\n"
     ]
    }
   ],
   "source": [
    "embd = make_embeddings(embeddings)\n",
    "if vector_store is not None:\n",
    "    vs = FAISS.load_local(vector_store, embd)\n",
    "    print(f\"Loaded {vector_store}\")\n",
    "else:\n",
    "    splits = []\n",
    "    for path in file_paths:\n",
    "        loader = PyMuPDFLoader(path) # Fast, Good for metadata\n",
    "        if split_method == \"RecursiveTextSplitter\":\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_chars,\n",
    "                                                           chunk_overlap=overlap)\n",
    "        elif split_method == \"CharacterTextSplitter\":\n",
    "            text_splitter = CharacterTextSplitter(separator=\" \",\n",
    "                                                  chunk_size=chunk_chars,\n",
    "                                                  chunk_overlap=overlap)\n",
    "        else:\n",
    "            raise Exception(\"Invalid text splitter\")\n",
    "    \n",
    "        local_pages = loader.load_and_split(text_splitter)\n",
    "        splits.extend(local_pages)\n",
    "    vs = FAISS.from_documents(splits, embd)\n",
    "    print(\"New vector store created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2696ab7",
   "metadata": {},
   "source": [
    "Save vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4598264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.save_local(\"vs_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb535de9",
   "metadata": {},
   "source": [
    "Run test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a939fb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making LLM\n",
      "Making chain\n",
      "Question 10 / 10, Correct: 6 / 9, ACC: 0.6666666666666666\n",
      "\n",
      "Total score: 0.7 (7 / 10)\n",
      "Avg. latency: 1.6796269416809082\n"
     ]
    }
   ],
   "source": [
    "retriever = vs.as_retriever(k=num_neighbors)\n",
    "results = run_evaluator(retriever, test_dataset, num_eval_questions, model_version, grade_prompt)\n",
    "score = results[\"answerScore\"].mean()\n",
    "score_str = f\"Total score: {score} ({len(results['answerScore'][results['answerScore'] == 1])} / {len(results['answerScore'])})\"\n",
    "print(\"\\n\")\n",
    "print(score_str)\n",
    "lat = results[results['latency'] < results['latency'].quantile(0.99)]['latency'].mean()\n",
    "print(f\"Avg. latency: {lat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a0543",
   "metadata": {},
   "source": [
    "Show incorrect examples first (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cf43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.index.name = \"idx\"\n",
    "results = results.sort_values(by = [\"answerScore\", \"idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6924eb6",
   "metadata": {},
   "source": [
    "Save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[score_str] = \"\"\n",
    "results[f\"Parameters: {str(params)}\"] = \"\"\n",
    "results.loc[results.index[0], score_str] = f\"Typical latency: {lat}\"\n",
    "results.to_csv(\"results/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd5d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
